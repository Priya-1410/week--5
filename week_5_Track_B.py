# -*- coding: utf-8 -*-
"""week_5_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P4X3V2hogDgvk6A9Z7MZhXYda6KoAdRU
"""

#@title 0) Setup & Load Images Metadata
import os, pandas as pd
from PIL import Image



meta = [
    {"image_id": "img1", "path": "/content/img_1.png", "caption": "Blue candlestick chart showing stock price movement over time."},
    {"image_id": "img2", "path": "/content/img_2.png", "caption": "Line chart illustrating upward trend in stock index."},
    {"image_id": "img3", "path": "/content/img_3.png", "caption": "Financial candlestick chart with volume overlay."},
    {"image_id": "img4", "path": "/content/img_4.png", "caption": "Combined line and candlestick chart of stock & trend indicator."}
]

df_imgs = pd.DataFrame(meta)
print("âœ… Loaded image metadata:", len(df_imgs))
display(df_imgs.head())

# 1) Load text corpus (from Track A CSV)
import pandas as pd
text_corpus = pd.read_csv("/content/corpus_chunks.csv")
text_corpus = text_corpus[['doc_id', 'text']]
print("âœ… Loaded text corpus:", text_corpus.shape)
display(text_corpus.head(3))

# 2) Load CLIP model (for both text + images)
from transformers import CLIPModel, CLIPProcessor
import torch
import numpy as np

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model.eval()

# 3) Build text embeddings
text_emb = {}
for _, row in text_corpus.iterrows():
    inputs = processor(text=row.text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        emb = clip_model.get_text_features(**inputs).squeeze().cpu().numpy()
    emb = emb / (np.linalg.norm(emb) + 1e-9)
    text_emb[row.doc_id] = emb.astype("float32")

print("âœ… Text embeddings built:", len(text_emb))

# 4) Build image embeddings
img_emb = {}
for _, row in df_imgs.iterrows():
    image = Image.open(row['path']).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        emb = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()
    emb = emb / (np.linalg.norm(emb) + 1e-9)
    img_emb[row.image_id] = emb.astype("float32")

print("âœ… Image embeddings built:", len(img_emb))

# 5) Cosine similarity + query encoder
def cosine(a,b):
    return float(a @ b / (np.linalg.norm(a)+1e-9) / (np.linalg.norm(b)+1e-9))

def encode_text(q):
    inputs = processor(text=q, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        emb = clip_model.get_text_features(**inputs).squeeze().cpu().numpy()
    emb = emb / (np.linalg.norm(emb) + 1e-9)
    return emb.astype("float32")

# 3) Retrieval Modes

def retrieve_text(query, k=3):
    q = encode_text(query)
    scores = [(doc, cosine(q, text_emb[doc])) for doc in text_emb]
    scores = sorted(scores, key=lambda x: -x[1])[:k]
    return [(doc, round(score, 3), text_corpus.loc[text_corpus['doc_id']==doc, 'text'].values[0]) for doc, score in scores]

def retrieve_image_by_text(query, k=3):
    q = encode_text(query)
    scores = [(img, cosine(q, img_emb[img])) for img in img_emb]
    scores = sorted(scores, key=lambda x: -x[1])[:k]
    return [(img, round(score, 3), df_imgs.loc[df_imgs['image_id']==img, 'caption'].values[0]) for img, score in scores]

def retrieve_by_image(image_id, k=3):
    q = img_emb[image_id]
    # Image -> Text
    t_scores = [(doc, cosine(q, text_emb[doc])) for doc in text_emb]
    t_scores = sorted(t_scores, key=lambda x: -x[1])[:k]
    top_text = [(doc, round(score, 3), text_corpus.loc[text_corpus['doc_id']==doc, 'text'].values[0]) for doc, score in t_scores]
    # Image -> Image
    i_scores = [(img, cosine(q, img_emb[img])) for img in img_emb if img != image_id]
    i_scores = sorted(i_scores, key=lambda x: -x[1])[:k]
    top_imgs = [(img, round(score, 3), df_imgs.loc[df_imgs['image_id']==img, 'caption'].values[0]) for img, score in i_scores]
    return top_text, top_imgs

# ðŸ”¹ Demo Run
print("\nText â†’ Docs")
for r in retrieve_text("How does sentiment correlate with stock prices?", 3):
    print(r)

print("\nText â†’ Images")
for r in retrieve_image_by_text("NASDAQ candlestick chart", 3):
    print(r)

print("\nImage â†’ Docs & Images (using img2)")
t_hits, i_hits = retrieve_by_image("img2", 3)
print("Imageâ†’Text results:")
for r in t_hits: print(r)
print("Imageâ†’Image results:")
for r in i_hits: print(r)

# 4) Prompt Assembly
def assemble_prompt(query, text_hits, image_hits):
    # text_hits is [(doc_id, score, snippet), ...]
    # image_hits is [(img_id, score, caption), ...]
    tbits = [f"[{doc}] {snippet}" for doc, _, snippet in text_hits]
    ibits = [f"[{img}] {caption}" for img, _, caption in image_hits]

    return f"""System: Answer using ONLY the evidence below. Cite [doc_id] or [image_id].
Query: {query}
Evidence:
- Text: {' | '.join(tbits)}
- Images: {' | '.join(ibits)}
Answer:
"""

# ðŸ”¹ Demo Run
q = "How does sentiment correlate with Tesla stock price?"
print(assemble_prompt(q, retrieve_text(q, 2), retrieve_image_by_text(q, 2)))

print('A) Text-only -> retrieve text + images-by-text')
q1 = "How does sentiment correlate with Tesla stock price?"
print(f"Query: {q1}\n")

print("Text results:")
for r in retrieve_text(q1, 3):
    print(r)

print("\nImages-from-text results:")
for r in retrieve_image_by_text(q1, 3):
    print(r)

print('\nB) Image-only -> retrieve related docs & similar images')
t_hits, i_hits = retrieve_by_image('img2', 3)

print("\nDocs related to img2:")
for r in t_hits:
    print(r)

print("\nSimilar images to img2:")
for r in i_hits:
    print(r)
